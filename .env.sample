#Ollama host configuration
OLLAMA_HOST="http://localhost:11434"

#LangSmith configuration
LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT='https://api.smith.langchain.com' #Enter your LangSmith endpoint if different
LANGCHAIN_API_KEY= #Enter your LangSmith API key here
LANGCHAIN_PROJECT= #Enter your LangSmith project name here

#LLM API configuration
LLM_BASE_URL = #Enter your API endpoint here
LLM_API_KEY = #Enter your API key provided by your inference provider here